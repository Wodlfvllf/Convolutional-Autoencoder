{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639ad0ea-8bdd-44ac-99f9-40c32080717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  article dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as tqdm_regular\n",
    "import seaborn as sns\n",
    "from torchvision.utils import make_grid\n",
    "import random\n",
    "from torchinfo import summary\n",
    " # Input shape (3 channels, 32x32 pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29234527-c7a8-4e08-83b5-1b1954c2d806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "#  configuring device\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00b26a3-ab96-4ef5-abdb-48986fa92bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import torchvision\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For descriptive error messages\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f985a44-be73-4a0b-a935-cebc54d54d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/21je0963/Competiton/data/new_train.csv\")\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b730c8a-7337-4454-bf3d-64dc84301be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "val_data = val_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1475e0dc-3b9d-4a54-9be9-7ba8eeb415c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CC', 'HGSC', 'EC', 'MC', 'LGSC'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in range \n",
    "train_data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f34cf79-22c8-466c-9f66-a55300f6edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = val_data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d62b59-f074-4a0d-8c83-ef9da2a55ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['EC', 'CC', 'HGSC', 'MC', 'LGSC'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0acc7387-5736-4e1e-929a-73fb116bc3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_labels = {index:labels for index, labels in enumerate(dictionary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d31e6d7-0f6a-497a-bc5c-8017a222dfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'EC', 1: 'CC', 2: 'HGSC', 3: 'MC', 4: 'LGSC'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fd68c6e-deb8-4906-87c8-c06589ac368b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EC': 0, 'CC': 1, 'HGSC': 2, 'MC': 3, 'LGSC': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_index = {labels:index for index, labels in enumerate(dictionary)}\n",
    "labels_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebb02deb-8e9d-4098-b5ee-5711c9dc1bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>img_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56947</td>\n",
       "      <td>CC</td>\n",
       "      <td>59288</td>\n",
       "      <td>24926</td>\n",
       "      <td>False</td>\n",
       "      <td>56947_thumbnail.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27747</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>47487</td>\n",
       "      <td>28122</td>\n",
       "      <td>False</td>\n",
       "      <td>27747_thumbnail.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5264</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>45054</td>\n",
       "      <td>22207</td>\n",
       "      <td>False</td>\n",
       "      <td>5264_thumbnail.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27739</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>80170</td>\n",
       "      <td>41514</td>\n",
       "      <td>False</td>\n",
       "      <td>27739_thumbnail.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32432</td>\n",
       "      <td>EC</td>\n",
       "      <td>35266</td>\n",
       "      <td>20507</td>\n",
       "      <td>False</td>\n",
       "      <td>32432_thumbnail.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma             img_path  \\\n",
       "0     56947    CC        59288         24926   False  56947_thumbnail.png   \n",
       "1     27747  HGSC        47487         28122   False  27747_thumbnail.png   \n",
       "2      5264  HGSC        45054         22207   False   5264_thumbnail.png   \n",
       "3     27739  HGSC        80170         41514   False  27739_thumbnail.png   \n",
       "4     32432    EC        35266         20507   False  32432_thumbnail.png   \n",
       "\n",
       "   fold  \n",
       "0     3  \n",
       "1     2  \n",
       "2     0  \n",
       "3     1  \n",
       "4     4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c23416a-a93e-4607-a746-8ab773451f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        ...,\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(df):\n",
    "    return labels_to_index[df]\n",
    "\n",
    "train_labels = train_data['label'].map(process)\n",
    "\n",
    "train_labels = list(train_labels)\n",
    "\n",
    "# train_labels\n",
    "color = torch.tensor(train_labels) # A tensor of categorical values\n",
    "\n",
    "one_hot_color = torch.nn.functional.one_hot(color, num_classes=5) # Converts the categorical tensor into a one-hot encoded tensor\n",
    "\n",
    "\n",
    "one_hot_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12a35339-0084-496e-bb71-6f0606cee262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>img_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56947</td>\n",
       "      <td>CC</td>\n",
       "      <td>59288</td>\n",
       "      <td>24926</td>\n",
       "      <td>False</td>\n",
       "      <td>56947_thumbnail.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27747</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>47487</td>\n",
       "      <td>28122</td>\n",
       "      <td>False</td>\n",
       "      <td>27747_thumbnail.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5264</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>45054</td>\n",
       "      <td>22207</td>\n",
       "      <td>False</td>\n",
       "      <td>5264_thumbnail.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27739</td>\n",
       "      <td>HGSC</td>\n",
       "      <td>80170</td>\n",
       "      <td>41514</td>\n",
       "      <td>False</td>\n",
       "      <td>27739_thumbnail.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32432</td>\n",
       "      <td>EC</td>\n",
       "      <td>35266</td>\n",
       "      <td>20507</td>\n",
       "      <td>False</td>\n",
       "      <td>32432_thumbnail.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id label  image_width  image_height  is_tma             img_path  \\\n",
       "0     56947    CC        59288         24926   False  56947_thumbnail.png   \n",
       "1     27747  HGSC        47487         28122   False  27747_thumbnail.png   \n",
       "2      5264  HGSC        45054         22207   False   5264_thumbnail.png   \n",
       "3     27739  HGSC        80170         41514   False  27739_thumbnail.png   \n",
       "4     32432    EC        35266         20507   False  32432_thumbnail.png   \n",
       "\n",
       "   fold  \n",
       "0     3  \n",
       "1     2  \n",
       "2     0  \n",
       "3     1  \n",
       "4     4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data = train_data.drop(columns = ['label'])\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b834e9cf-d004-4105-82a4-a7eafd6782ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_id = train_data['image_id']\n",
    "\n",
    "train_data['label'][0] = one_hot_color[0]\n",
    "\n",
    "for i, vect in enumerate(one_hot_color):\n",
    "    train_data['label'][i] = vect\n",
    "\n",
    "train_data = train_data.drop(columns = ['image_width', 'image_height','is_tma'])\n",
    "train_data.head()\n",
    "\n",
    "train_data['label'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4099468f-8156-47f1-817e-aabfd7c2b4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>img_path</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6363</td>\n",
       "      <td>[tensor(1), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>6363_thumbnail.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53098</td>\n",
       "      <td>[tensor(1), tensor(0), tensor(0), tensor(0), t...</td>\n",
       "      <td>53098_thumbnail.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28603</td>\n",
       "      <td>[tensor(0), tensor(1), tensor(0), tensor(0), t...</td>\n",
       "      <td>28603_thumbnail.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30794</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(1), tensor(0), t...</td>\n",
       "      <td>30794_thumbnail.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11559</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(1), tensor(0), t...</td>\n",
       "      <td>11559_thumbnail.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                              label  \\\n",
       "0      6363  [tensor(1), tensor(0), tensor(0), tensor(0), t...   \n",
       "1     53098  [tensor(1), tensor(0), tensor(0), tensor(0), t...   \n",
       "2     28603  [tensor(0), tensor(1), tensor(0), tensor(0), t...   \n",
       "3     30794  [tensor(0), tensor(0), tensor(1), tensor(0), t...   \n",
       "4     11559  [tensor(0), tensor(0), tensor(1), tensor(0), t...   \n",
       "\n",
       "              img_path  fold  \n",
       "0   6363_thumbnail.png     2  \n",
       "1  53098_thumbnail.png     2  \n",
       "2  28603_thumbnail.png     1  \n",
       "3  30794_thumbnail.png     2  \n",
       "4  11559_thumbnail.png     1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(df):\n",
    "    return labels_to_index[df]\n",
    "\n",
    "val_labels = val_data['label'].map(process)\n",
    "\n",
    "val_labels = list(val_labels)\n",
    "\n",
    "# train_labels\n",
    "color = torch.tensor(val_labels) # A tensor of categorical values\n",
    "\n",
    "val_one_hot_color = torch.nn.functional.one_hot(color, num_classes=5) # Converts the categorical tensor into a one-hot encoded tensor\n",
    "\n",
    "\n",
    "# val_one_hot_color\n",
    "\n",
    "img_id = val_data['image_id']\n",
    "\n",
    "val_data['label'][0] = val_one_hot_color[0]\n",
    "\n",
    "for i, vect in enumerate(val_one_hot_color):\n",
    "    val_data['label'][i] = vect\n",
    "\n",
    "val_data = val_data.drop(columns = ['image_width', 'image_height','is_tma'])\n",
    "val_data.head()\n",
    "\n",
    "# # train_data['labels'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b75eb605-e8dc-4882-b0c0-2836a4ca63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, data_dir, transform=None):\n",
    "        # Load the CSV file with image filenames and labels\n",
    "        self.data = data\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the image filename and label from the CSV data\n",
    "        img_filename = os.path.join(self.data_dir, self.data['img_path'][index])\n",
    "        label = self.data['label'][index]\n",
    "        \n",
    "        with Image.open(img_filename) as img:\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "data_dir = '/home/21je0963/Competiton/data/train_img'\n",
    "# csv_file = 'your_csv_file.csv'  # Replace with the actual CSV file path\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((768, 768)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create instances of your custom dataset for training and validation\n",
    "train_custom_dataset = CustomDataset(train_data, data_dir, transform=transform)\n",
    "\n",
    "# Assuming you have a separate validation CSV file, you can create a validation dataset like this:\n",
    "# val_csv_file = 'your_validation_csv_file.csv'  # Replace with the actual CSV file path\n",
    "val_custom_dataset = CustomDataset(val_data, data_dir, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9191b5c9-b967-4654-96c7-cf5e6351745f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 4  # Specify your desired batch size\n",
    "data_loader = DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "for batch in data_loader:\n",
    "    images, labels = batch\n",
    "    break\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a9a67b0-c65e-4268-ae81-a94de804ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder = Encoder()\n",
    "# encoder.to(device)\n",
    "# summary(encoder, (3, 32, 32), device = device) \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),  # (768, 768)\n",
    "            act_fn,\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            act_fn,\n",
    "            nn.Conv2d(out_channels, 2*out_channels, 3, padding=1, stride=2),  # (384, 384)\n",
    "            act_fn,\n",
    "            nn.Conv2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "            act_fn,\n",
    "            nn.Conv2d(2*out_channels, 4*out_channels, 3, padding=1, stride=2),  # (192, 192)\n",
    "            act_fn,\n",
    "            nn.Conv2d(4*out_channels, 4*out_channels, 3, padding=1),\n",
    "            act_fn,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4*out_channels*192*192, latent_dim),\n",
    "            act_fn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 768, 768)\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 4 * out_channels * 192 * 192),\n",
    "            act_fn\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4 * out_channels, 4 * out_channels, 3, padding=1),  # (192, 192)\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(4 * out_channels, 2 * out_channels, 3, padding=1, stride=2, output_padding=1),  # (384, 384)\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(2 * out_channels, 2 * out_channels, 3, padding=1),\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(2 * out_channels, out_channels, 3, padding=1, stride=2, output_padding=1),  # (768, 768)\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(out_channels, out_channels, 3, padding=1),\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(out_channels, in_channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        output = output.view(-1, 4 * self.out_channels, 192, 192)\n",
    "        output = self.conv(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2a0d974-d6af-463a-8d57-65df2e405fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('mse_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b1a810b-6fb8-4366-9294-cf4cc9ca0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21d016b2-dcc9-47c7-b3ee-744de9e3ecec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Encoder                                  [1, 200]                  --\n",
       "├─Sequential: 1-1                        [1, 200]                  --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 768, 768]         448\n",
       "│    └─ReLU: 2-2                         [1, 16, 768, 768]         --\n",
       "│    └─Conv2d: 2-3                       [1, 16, 768, 768]         2,320\n",
       "│    └─ReLU: 2-4                         [1, 16, 768, 768]         --\n",
       "│    └─Conv2d: 2-5                       [1, 32, 384, 384]         4,640\n",
       "│    └─ReLU: 2-6                         [1, 32, 384, 384]         --\n",
       "│    └─Conv2d: 2-7                       [1, 32, 384, 384]         9,248\n",
       "│    └─ReLU: 2-8                         [1, 32, 384, 384]         --\n",
       "│    └─Conv2d: 2-9                       [1, 64, 192, 192]         18,496\n",
       "│    └─ReLU: 2-10                        [1, 64, 192, 192]         --\n",
       "│    └─Conv2d: 2-11                      [1, 64, 192, 192]         36,928\n",
       "│    └─ReLU: 2-12                        [1, 64, 192, 192]         --\n",
       "│    └─Flatten: 2-13                     [1, 2359296]              --\n",
       "│    └─Linear: 2-14                      [1, 200]                  471,859,400\n",
       "│    └─ReLU: 2-15                        [1, 200]                  --\n",
       "==========================================================================================\n",
       "Total params: 471,931,480\n",
       "Trainable params: 471,931,480\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.20\n",
       "==========================================================================================\n",
       "Input size (MB): 7.08\n",
       "Forward/backward pass size (MB): 264.24\n",
       "Params size (MB): 1887.73\n",
       "Estimated Total Size (MB): 2159.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(encoder, (3,768,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c30a8aac-fcfc-49ac-98b9-f7b74792fbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 5]                    --\n",
       "├─Encoder: 1-1                           [1, 200]                  --\n",
       "│    └─Sequential: 2-1                   [1, 200]                  --\n",
       "│    │    └─Conv2d: 3-1                  [1, 16, 768, 768]         (448)\n",
       "│    │    └─ReLU: 3-2                    [1, 16, 768, 768]         --\n",
       "│    │    └─Conv2d: 3-3                  [1, 16, 768, 768]         (2,320)\n",
       "│    │    └─ReLU: 3-4                    [1, 16, 768, 768]         --\n",
       "│    │    └─Conv2d: 3-5                  [1, 32, 384, 384]         (4,640)\n",
       "│    │    └─ReLU: 3-6                    [1, 32, 384, 384]         --\n",
       "│    │    └─Conv2d: 3-7                  [1, 32, 384, 384]         (9,248)\n",
       "│    │    └─ReLU: 3-8                    [1, 32, 384, 384]         --\n",
       "│    │    └─Conv2d: 3-9                  [1, 64, 192, 192]         (18,496)\n",
       "│    │    └─ReLU: 3-10                   [1, 64, 192, 192]         --\n",
       "│    │    └─Conv2d: 3-11                 [1, 64, 192, 192]         (36,928)\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 192, 192]         --\n",
       "│    │    └─Flatten: 3-13                [1, 2359296]              --\n",
       "│    │    └─Linear: 3-14                 [1, 200]                  (471,859,400)\n",
       "│    │    └─ReLU: 3-15                   [1, 200]                  --\n",
       "├─Sequential: 1-2                        [1, 5]                    --\n",
       "│    └─Linear: 2-2                       [1, 128]                  25,728\n",
       "│    └─ReLU: 2-3                         [1, 128]                  --\n",
       "│    └─Linear: 2-4                       [1, 5]                    645\n",
       "==========================================================================================\n",
       "Total params: 471,957,853\n",
       "Trainable params: 26,373\n",
       "Non-trainable params: 471,931,480\n",
       "Total mult-adds (G): 6.20\n",
       "==========================================================================================\n",
       "Input size (MB): 7.08\n",
       "Forward/backward pass size (MB): 264.24\n",
       "Params size (MB): 1887.83\n",
       "Estimated Total Size (MB): 2159.15\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "classification_head = nn.Sequential(\n",
    "    nn.Linear(200, 128),  # Adjust num_hidden_units as needed\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 5)  # Adjust num_classes as needed\n",
    ")\n",
    "\n",
    "classification_model = nn.Sequential(\n",
    "    encoder,\n",
    "    classification_head\n",
    ")\n",
    "\n",
    "summary(classification_model, (3, 768, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8db0d82d-2868-4d6c-a9d7-bc0956701b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder():\n",
    "  def __init__(self, autoencoder):\n",
    "    self.network = autoencoder\n",
    "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=1e-3)\n",
    "    self.file1 = open(\"Train_logs_classification.txt\",'w')\n",
    "    self.file1.write(\"Training_started..............\\n\")\n",
    "    self.file1.close()\n",
    "      \n",
    "    \n",
    "  def train(self, loss_function, epochs, batch_size, training_set, validation_set):\n",
    "    #  creating log\n",
    "    log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'visualizations': []\n",
    "    } \n",
    "\n",
    "    #  defining weight initialization function\n",
    "    def init_weights(module):\n",
    "      if isinstance(module, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "      elif isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "\n",
    "    #  initializing network weights\n",
    "    self.network.apply(init_weights)\n",
    "\n",
    "    #  creating dataloaders\n",
    "    \n",
    "    train_loader = DataLoader(training_set, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(validation_set, batch_size=8, shuffle=True)\n",
    "    # test_loader = DataLoader(test_dataset, 10)\n",
    "    #  setting convnet to training mode\n",
    "    self.network.train()\n",
    "    self.network.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #------------\n",
    "      #  TRAINING\n",
    "      #------------\n",
    "      print('training...')\n",
    "      for images, labels in tqdm(train_loader):\n",
    "        #  zeroing gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        #  sending images to device\n",
    "        # print(len(images))\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels.float()\n",
    "        #  reconstructing images\n",
    "        output = self.network(images)\n",
    "        # print(output.shape)\n",
    "        #  computing loss\n",
    "        loss = loss_function(output, labels)\n",
    "        #  calculating gradients\n",
    "        loss.backward()\n",
    "        #  optimizing weights\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #--------------\n",
    "        # LOGGING\n",
    "        #--------------\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "\n",
    "      #--------------\n",
    "      # VALIDATION\n",
    "      #--------------\n",
    "      print('validating...')\n",
    "      for val_images, val_labels in tqdm(val_loader):\n",
    "      \n",
    "        with torch.no_grad():\n",
    "          #  sending validation images to device\n",
    "          val_images = val_images.to(device)\n",
    "          val_labels = val_labels.to(device)\n",
    "          val_labels = val_labels.float()\n",
    "          #  reconstructing images\n",
    "          output = self.network(val_images)\n",
    "          #  computing validation loss\n",
    "          val_loss = loss_function(output, val_labels)\n",
    "\n",
    "        #--------------\n",
    "        # LOGGING\n",
    "        #--------------\n",
    "        log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "      \n",
    "     \n",
    "\n",
    "      #--------------\n",
    "      # VISUALISATION\n",
    "      #--------------\n",
    "      print(f'training_loss: {round(loss.item(), 4)} validation_loss: {round(val_loss.item(), 4)}\\n')\n",
    "      l = f'training_loss: {round(loss.item(), 4)} validation_loss: {round(val_loss.item(), 4)}\\n'\n",
    "      self.file1 = open(\"Train_logs_classification.txt\",'a')\n",
    "      self.file1.write(l)\n",
    "      self.file1.close()\n",
    "      # for test_images in test_loader:\n",
    "      #   #  sending test images to device\n",
    "      #   test_images = test_images.to(device)\n",
    "      #   with torch.no_grad():\n",
    "      #     #  reconstructing test images\n",
    "      #     reconstructed_imgs = self.network(test_images)\n",
    "      #   #  sending reconstructed and images to cpu to allow for visualization\n",
    "      #   reconstructed_imgs = reconstructed_imgs.cpu()\n",
    "      #   test_images = test_images.cpu()\n",
    "\n",
    "      #   #  visualisation\n",
    "      #   imgs = torch.stack([test_images.view(-1, 3, 768, 768), reconstructed_imgs], \n",
    "      #                     dim=1).flatten(0,1)\n",
    "      #   grid = make_grid(imgs, nrow=10, normalize=True, padding=1)\n",
    "      #   grid = grid.permute(1, 2, 0)\n",
    "      #   plt.figure(dpi=170)\n",
    "      #   plt.title('Original/Reconstructed')\n",
    "      #   plt.imshow(grid)\n",
    "      #   log_dict['visualizations'].append(grid)\n",
    "      #   plt.axis('off')\n",
    "      #   plt.show()\n",
    "    \n",
    "    # self.file1.close() \n",
    "    return log_dict\n",
    "\n",
    "  def autoencode(self, x):\n",
    "    return self.network(x)\n",
    "\n",
    "  def encode(self, x):\n",
    "    encoder = self.network.encoder\n",
    "    return encoder(x)\n",
    "  \n",
    "  def decode(self, x):\n",
    "    decoder = self.network.decoder\n",
    "    return decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7001551a-a88a-4ebd-80fb-a922b361af97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Encoder(\n",
       "    (net): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU()\n",
       "      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (9): ReLU()\n",
       "      (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU()\n",
       "      (12): Flatten(start_dim=1, end_dim=-1)\n",
       "      (13): Linear(in_features=2359296, out_features=200, bias=True)\n",
       "      (14): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dd9a53b-7e7f-468f-9d4f-c04edd9a09bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2500,  1.3625, -1.0828, -0.2255, -3.7909]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Image.open('/home/21je0963/Competiton/data/train_img/56947_thumbnail.png') as imgg:\n",
    "    imgg = transform(imgg).to(device)\n",
    "    output = classification_model(imgg)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ee32c7f-a91b-428d-b6dc-8fe8e9ef618f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29feb894-13f7-4ecd-8284-5651c78447d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [01:40<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:24<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.1397 validation_loss: 0.1137\n",
      "\n",
      "Epoch 2/10\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [01:30<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:24<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.1431 validation_loss: 0.1479\n",
      "\n",
      "Epoch 3/10\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [01:35<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:24<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss: 0.1576 validation_loss: 0.1438\n",
      "\n",
      "Epoch 4/10\n",
      "training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [01:32<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [00:05<00:34,  2.84s/it]"
     ]
    }
   ],
   "source": [
    "modell = ConvolutionalAutoencoder(classification_model)\n",
    "log_dict = modell.train(nn.MSELoss(), epochs=10, batch_size=4, training_set=train_custom_dataset, validation_set=val_custom_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b6cf9-7b18-4198-b9df-275aa7e8e7c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393c1a7-74e4-4872-b584-bd67c31ac014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
