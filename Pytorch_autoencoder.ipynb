{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ea975-36c2-429f-9b8e-827870d1a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0f3172-a5d4-4ad3-abf4-6e5920b176b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8024931b-3de1-46c9-b068-e69cb12fbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ac0a8-dc16-48df-abb1-b81f7f6deb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59dffac-c3bf-43c2-9a85-ab71615a128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummarypip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b15cb86-fc61-4a25-9491-27937c672f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83351890-ac6d-4b85-acb6-5caf67cd5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc45026b-4e3e-424f-8a2b-65072f0f6a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49400749-d63b-43fe-adfe-63b5f05942d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  article dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as tqdm_regular\n",
    "import seaborn as sns\n",
    "from torchvision.utils import make_grid\n",
    "import random\n",
    "from torchinfo import summary\n",
    " # Input shape (3 channels, 32x32 pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7044a8-d575-4eec-a56f-f7b7368bb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  configuring device\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea547ce-77e7-4be5-b401-7682ae440266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import torchvision\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401cc082-0e4e-465c-a376-e484169fe118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "train = []\n",
    "size = (1536, 1536)\n",
    "\n",
    "for filename in os.listdir(\"/home/21je0963/Competiton/data/train_img\"):\n",
    "    train.append(filename)\n",
    "\n",
    "val_data = train[:100]\n",
    "train_data = train[101:]\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, list_IDs, data_dir, transform=None):\n",
    "        self.list_IDs = list_IDs\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ID = self.list_IDs[index]\n",
    "        img_path = os.path.join(self.data_dir, ID)\n",
    "\n",
    "        with Image.open(img_path) as img:\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "data_dir = '/home/21je0963/Competiton/data/train_img/'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((768, 768)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_custom_dataset = CustomDataset(train_data, data_dir, transform=transform)\n",
    "val_custom_dataset = CustomDataset(val_data, data_dir, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b76c9-7d60-49c2-9d94-c6034815afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UBCDataset(Dataset):\n",
    "#     def __init__(self, df, transforms=None):\n",
    "#         self.df = df\n",
    "#         self.file_names = df['file_path'].values\n",
    "#         self.labels = df['label'].values\n",
    "#         self.transforms = transforms\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         img_path = self.file_names[index]\n",
    "#         img = cv2.imread(img_path)\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         label = self.labels[index]\n",
    "        \n",
    "#         if self.transforms:\n",
    "#             img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "#         return {\n",
    "#             'image': img,\n",
    "#             'label': torch.tensor(label, dtype=torch.long)\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe819d-4bb3-48bd-b660-a2825f08bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  defining encoder\n",
    "# class Encoder(nn.Module):\n",
    "#   def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "#     super().__init__()\n",
    "\n",
    "#     self.net = nn.Sequential(\n",
    "#         nn.Conv2d(in_channels, out_channels, 3, padding=1), # (32, 32)\n",
    "#         act_fn,\n",
    "#         nn.Conv2d(out_channels, out_channels, 3, padding=1), \n",
    "#         act_fn,\n",
    "#         nn.Conv2d(out_channels, 2*out_channels, 3, padding=1, stride=2), # (16, 16)\n",
    "#         act_fn,\n",
    "#         nn.Conv2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "#         act_fn,\n",
    "#         nn.Conv2d(2*out_channels, 4*out_channels, 3, padding=1, stride=2), # (8, 8)\n",
    "#         act_fn,\n",
    "#         nn.Conv2d(4*out_channels, 4*out_channels, 3, padding=1),\n",
    "#         act_fn,\n",
    "#         nn.Flatten(),\n",
    "#         nn.Linear(4*out_channels*8*8, latent_dim),\n",
    "#         act_fn\n",
    "#     )\n",
    "\n",
    "#   def forward(self, x):\n",
    "#     x = x.view(-1, 3, 32, 32)\n",
    "#     output = self.net(x)\n",
    "#     return output\n",
    "\n",
    "\n",
    "# #  defining decoder\n",
    "# class Decoder(nn.Module):\n",
    "#   def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "#     super().__init__()\n",
    "\n",
    "#     self.out_channels = out_channels\n",
    "\n",
    "#     self.linear = nn.Sequential(\n",
    "#         nn.Linear(latent_dim, 4*out_channels*8*8),\n",
    "#         act_fn\n",
    "#     )\n",
    "\n",
    "#     self.conv = nn.Sequential(\n",
    "#         nn.ConvTranspose2d(4*out_channels, 4*out_channels, 3, padding=1), # (8, 8)\n",
    "#         act_fn,\n",
    "#         nn.ConvTranspose2d(4*out_channels, 2*out_channels, 3, padding=1, \n",
    "#                            stride=2, output_padding=1), # (16, 16)\n",
    "#         act_fn,\n",
    "#         nn.ConvTranspose2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "#         act_fn,\n",
    "#         nn.ConvTranspose2d(2*out_channels, out_channels, 3, padding=1, \n",
    "#                            stride=2, output_padding=1), # (32, 32)\n",
    "#         act_fn,\n",
    "#         nn.ConvTranspose2d(out_channels, out_channels, 3, padding=1),\n",
    "#         act_fn,\n",
    "#         nn.ConvTranspose2d(out_channels, in_channels, 3, padding=1)\n",
    "#     )\n",
    "\n",
    "#   def forward(self, x):\n",
    "#     output = self.linear(x)\n",
    "#     output = output.view(-1, 4*self.out_channels, 8, 8)\n",
    "#     output = self.conv(output)\n",
    "#     return output\n",
    "\n",
    "\n",
    "# #  defining autoencoder\n",
    "# class Autoencoder(nn.Module):\n",
    "#   def __init__(self, encoder, decoder):\n",
    "#     super().__init__()\n",
    "#     self.encoder = encoder\n",
    "#     self.encoder.to(device)\n",
    "\n",
    "#     self.decoder = decoder\n",
    "#     self.decoder.to(device)\n",
    "\n",
    "#   def forward(self, x):\n",
    "#     encoded = self.encoder(x)\n",
    "#     decoded = self.decoder(encoded)\n",
    "#     return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2323ea42-5d13-4837-8b68-af1b2a8458f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e109f-b33a-4bb8-a89b-28b675a727ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder = Encoder()\n",
    "# encoder.to(device)\n",
    "# summary(encoder, (3, 32, 32), device = device) \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),  # (768, 768)\n",
    "            act_fn,\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            act_fn,\n",
    "            nn.Conv2d(out_channels, 2*out_channels, 3, padding=1, stride=2),  # (384, 384)\n",
    "            act_fn,\n",
    "            nn.Conv2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "            act_fn,\n",
    "            nn.Conv2d(2*out_channels, 4*out_channels, 3, padding=1, stride=2),  # (192, 192)\n",
    "            act_fn,\n",
    "            nn.Conv2d(4*out_channels, 4*out_channels, 3, padding=1),\n",
    "            act_fn,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4*out_channels*192*192, latent_dim),\n",
    "            act_fn\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3, 768, 768)\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 4 * out_channels * 192 * 192),\n",
    "            act_fn\n",
    "        )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4 * out_channels, 4 * out_channels, 3, padding=1),  # (192, 192)\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(4 * out_channels, 2 * out_channels, 3, padding=1, stride=2, output_padding=1),  # (384, 384)\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(2 * out_channels, 2 * out_channels, 3, padding=1),\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(2 * out_channels, out_channels, 3, padding=1, stride=2, output_padding=1),  # (768, 768)\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(out_channels, out_channels, 3, padding=1),\n",
    "            act_fn,\n",
    "            nn.ConvTranspose2d(out_channels, in_channels, 3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        output = output.view(-1, 4 * self.out_channels, 192, 192)\n",
    "        output = self.conv(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5db9d2-bc6f-4e3a-8885-cea87f562be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "encoder.to(device)\n",
    "summary(encoder, (3, 768, 768), device = device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b5656-47e8-4dea-acca-29579c7b9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder()\n",
    "decoder.to(device)\n",
    "summary(decoder, (1,200), device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d4068-15ef-4ccc-8df4-289619384155",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(encoder, decoder)\n",
    "autoencoder.to(device)\n",
    "summary(autoencoder, (3,1536, 1536), device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d435def-0c06-4aab-8242-ab33fa66c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(1)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676022f-87d5-4ced-a650-1148376379a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a custom Laplacian filter bank layer.\n",
    "class LaplacianFilterBank(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, filter_sizes):\n",
    "        super(LaplacianFilterBank, self).__init__()\n",
    "\n",
    "        self.filters = nn.ModuleList()\n",
    "        for filter_size in filter_sizes:\n",
    "            self.filters.append(nn.Conv2d(in_channels, num_filters, filter_size, padding=(filter_size - 1) // 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        subbands = [filter(x) for filter in self.filters]\n",
    "        return subbands\n",
    "\n",
    "# Custom spatial frequency loss module.\n",
    "class SpatialFrequencyLoss(nn.Module):\n",
    "    def __init__(self, in_channels, num_filters, filter_sizes):\n",
    "        super(SpatialFrequencyLoss, self).__init__()\n",
    "        self.filter_bank = LaplacianFilterBank(in_channels, num_filters, filter_sizes).to(device)\n",
    "\n",
    "    def forward(self, input_image, reconstructed_image):\n",
    "        input_subbands = self.filter_bank(input_image)\n",
    "        reconstructed_subbands = self.filter_bank(reconstructed_image)\n",
    "\n",
    "        # Compute the MSE loss for each subband and sum them up.\n",
    "        spatial_losses = [F.mse_loss(reconstructed_subband, input_subband) for reconstructed_subband, input_subband in zip(reconstructed_subbands, input_subbands)]\n",
    "        spatial_frequency_loss = torch.sum(torch.stack(spatial_losses))\n",
    "\n",
    "        return spatial_frequency_loss\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "# input_image = torch.randn(1, 3, 768, 768)  # Replace with your actual input image\n",
    "# reconstructed_image = torch.randn(1, 3, 768, 768)  # Replace with your actual reconstructed image\n",
    "\n",
    "# loss = spatial_loss(input_image, reconstructed_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcaf07-b963-445b-8246-72ac9f68fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder():\n",
    "  def __init__(self, autoencoder):\n",
    "    self.network = autoencoder\n",
    "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=1e-3)\n",
    "    self.file1 = open(\"Train_logs_Spatial.txt\",'w')\n",
    "    self.file1.write(\"Training_started..............\")\n",
    "    self.file1.close()\n",
    "      \n",
    "    \n",
    "  def train(self, loss_function, epochs, batch_size, training_set, validation_set, test_dataset):\n",
    "    #  creating log\n",
    "    log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'visualizations': []\n",
    "    } \n",
    "\n",
    "    #  defining weight initialization function\n",
    "    def init_weights(module):\n",
    "      if isinstance(module, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "      elif isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "\n",
    "    #  initializing network weights\n",
    "    self.network.apply(init_weights)\n",
    "\n",
    "    #  creating dataloaders\n",
    "    \n",
    "    train_loader = DataLoader(training_set, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(validation_set, batch_size=8, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, 10)\n",
    "    #  setting convnet to training mode\n",
    "    self.network.train()\n",
    "    self.network.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #------------\n",
    "      #  TRAINING\n",
    "      #------------\n",
    "      print('training...')\n",
    "      for images in tqdm(train_loader):\n",
    "        #  zeroing gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        #  sending images to device\n",
    "        # print(len(images))\n",
    "        images = images.to(device)\n",
    "          \n",
    "        #  reconstructing images\n",
    "        output = self.network(images)\n",
    "        #  computing loss\n",
    "        loss = loss_function(output, images.view(-1, 3, 768, 768))\n",
    "        #  calculating gradients\n",
    "        loss.backward()\n",
    "        #  optimizing weights\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #--------------\n",
    "        # LOGGING\n",
    "        #--------------\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "\n",
    "      #--------------\n",
    "      # VALIDATION\n",
    "      #--------------\n",
    "      print('validating...')\n",
    "      for val_images in tqdm(val_loader):\n",
    "      \n",
    "        with torch.no_grad():\n",
    "          #  sending validation images to device\n",
    "          val_images = val_images.to(device)\n",
    "          #  reconstructing images\n",
    "          output = self.network(val_images)\n",
    "          #  computing validation loss\n",
    "          val_loss = loss_function(output, val_images.view(-1, 3, 768, 768))\n",
    "\n",
    "        #--------------\n",
    "        # LOGGING\n",
    "        #--------------\n",
    "        log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "      \n",
    "     \n",
    "\n",
    "      #--------------\n",
    "      # VISUALISATION\n",
    "      #--------------\n",
    "      print(f'training_loss: {round(loss.item(), 4)} validation_loss: {round(val_loss.item(), 4)}')\n",
    "      l = f'training_loss: {round(loss.item(), 4)} validation_loss: {round(val_loss.item(), 4)}'\n",
    "      self.file1 = open(\"Train_logs_Spatial.txt\",'a')\n",
    "      self.file1.write(l)\n",
    "      self.file1.close()\n",
    "      for test_images in test_loader:\n",
    "        #  sending test images to device\n",
    "        test_images = test_images.to(device)\n",
    "        with torch.no_grad():\n",
    "          #  reconstructing test images\n",
    "          reconstructed_imgs = self.network(test_images)\n",
    "        #  sending reconstructed and images to cpu to allow for visualization\n",
    "        reconstructed_imgs = reconstructed_imgs.cpu()\n",
    "        test_images = test_images.cpu()\n",
    "\n",
    "        #  visualisation\n",
    "        imgs = torch.stack([test_images.view(-1, 3, 768, 768), reconstructed_imgs], \n",
    "                          dim=1).flatten(0,1)\n",
    "        grid = make_grid(imgs, nrow=10, normalize=True, padding=1)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(dpi=170)\n",
    "        plt.title('Original/Reconstructed')\n",
    "        plt.imshow(grid)\n",
    "        log_dict['visualizations'].append(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    # self.file1.close() \n",
    "    return log_dict\n",
    "\n",
    "  def autoencode(self, x):\n",
    "    return self.network(x)\n",
    "\n",
    "  def encode(self, x):\n",
    "    encoder = self.network.encoder\n",
    "    return encoder(x)\n",
    "  \n",
    "  def decode(self, x):\n",
    "    decoder = self.network.decoder\n",
    "    return decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57160dd-745f-4f09-86ec-a60bf073462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ConvolutionalAutoencoder(Autoencoder(Encoder(), Decoder()))\n",
    "# model.summary()\n",
    "test_dataset = CustomDataset(val_data[:10], data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342cd6e1-bfac-4d0b-97d1-ac66c3a14df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3  # Input has 3 channels\n",
    "num_filters = 16  # Number of filters in the filter bank\n",
    "filter_sizes = [3, 5]  # Filter sizes to capture different spatial frequencies\n",
    "\n",
    "loss_function = SpatialFrequencyLoss(in_channels, num_filters, filter_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c36d50-c4a7-40ce-9b12-2ee69cf7ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalAutoencoder(autoencoder)\n",
    "\n",
    "log_dict = model.train(loss_function, epochs=100, batch_size=4, training_set=train_custom_dataset, validation_set=val_custom_dataset, test_dataset = test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409ecf6-9bed-464f-8bc4-a713a5a613ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(autoencoder, 'spatial_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95972d59-70a7-4397-8e46-656366c1e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "modell = torch.load('spatial_model_weights.pth')\n",
    "\n",
    "# summary(model, (3, 768, 768))\n",
    "test_loader = DataLoader(test_dataset, 10)\n",
    "\n",
    "for test_images in test_loader:\n",
    "        #  sending test images to device\n",
    "        test_images = test_images.to(device)\n",
    "        with torch.no_grad():\n",
    "          #  reconstructing test images\n",
    "          reconstructed_imgs = modell(test_images)\n",
    "        #  sending reconstructed and images to cpu to allow for visualization\n",
    "        reconstructed_imgs = reconstructed_imgs.cpu()\n",
    "        test_images = test_images.cpu()\n",
    "\n",
    "        #  visualisation\n",
    "        imgs = torch.stack([test_images.view(-1, 3, 768, 768), reconstructed_imgs], \n",
    "                          dim=1).flatten(0,1)\n",
    "        grid = make_grid(imgs, nrow=10, normalize=True, padding=1)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(dpi=170)\n",
    "        plt.title('Original/Reconstructed')\n",
    "        plt.imshow(grid)\n",
    "        log_dict['visualizations'].append(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e3c40-0755-4676-91e5-d7fa540f9c05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
